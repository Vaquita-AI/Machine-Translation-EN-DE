{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e7877d",
   "metadata": {
    "id": "82e7877d"
   },
   "source": [
    "# Machine Translation (EN to DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367b361",
   "metadata": {
    "id": "d367b361"
   },
   "source": [
    "# Google Colab/Local Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a17c9",
   "metadata": {
    "id": "b34a17c9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import torch\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "# path to the link you created\n",
    "en_file = '/content/drive/MyDrive/Machine Translation DE > EN/europarl-v7.en_20k_lines.txt'\n",
    "de_file = '/content/drive/MyDrive/Machine Translation DE > EN/europarl-v7.de_20k_lines.txt'\n",
    "val_en_file = '/content/drive/MyDrive/Machine Translation DE > EN/europarl-v7.en_tiny_lines.txt'\n",
    "val_de_file = '/content/drive/MyDrive/Machine Translation DE > EN/europarl-v7.de_tiny_lines.txt'\n",
    "test_file = '/content/drive/MyDrive/Machine Translation DE > EN/test-en.txt'\n",
    "\n",
    "# Model save path\n",
    "model_save_dir = '/content/drive/MyDrive/MachineTranslationModels'\n",
    "# Use os.path.join to create the full path for the model checkpoint\n",
    "model_save_path = os.path.join(model_save_dir, 'transformer_best.pth')\n",
    "\n",
    "# Set the device to the TPU\n",
    "#device = xm.xla_device()\n",
    "# Set the device to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487341ab",
   "metadata": {
    "id": "487341ab"
   },
   "outputs": [],
   "source": [
    "!pip install cloud-tpu-client\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install torch-xla\n",
    "!pip install spacy\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install --upgrade torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4fb8c7",
   "metadata": {
    "id": "4f4fb8c7"
   },
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7b404e",
   "metadata": {
    "id": "3b7b404e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, model_dim, head_count):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        assert model_dim % head_count == 0, \"model_dim must be divisible by head_count\"\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.head_count = head_count\n",
    "        self.depth = model_dim // head_count\n",
    "\n",
    "        self.query_weight = nn.Linear(model_dim, model_dim)\n",
    "        self.key_weight = nn.Linear(model_dim, model_dim)\n",
    "        self.value_weight = nn.Linear(model_dim, model_dim)\n",
    "        self.output_weight = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.depth)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        probabilities = torch.softmax(scores, dim=-1)\n",
    "        attention_output = torch.matmul(probabilities, value)\n",
    "        return attention_output\n",
    "\n",
    "    def split(self, tensor):\n",
    "        batch_size, sequence_length, model_dim = tensor.size()\n",
    "        return tensor.view(batch_size, sequence_length, self.head_count, self.depth).transpose(1, 2)\n",
    "\n",
    "    def combine(self, tensor):\n",
    "        batch_size, _, sequence_length, depth = tensor.size()\n",
    "        return tensor.transpose(1, 2).contiguous().view(batch_size, sequence_length, self.model_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.split(self.query_weight(query))\n",
    "        key = self.split(self.key_weight(key))\n",
    "        value = self.split(self.value_weight(value))\n",
    "\n",
    "        attention = self.attention(query, key, value, mask)\n",
    "        combined_attention = self.combine(attention)\n",
    "        output = self.output_weight(combined_attention)\n",
    "        return output\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, model_dim, ff_dim):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, model_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return self.linear2(self.relu(self.linear1(tensor)))\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, model_dim):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        batch_size, sequence_length = tensor.size(0), tensor.size(1)\n",
    "        position_encoding = torch.zeros(sequence_length, self.model_dim, device=tensor.device)\n",
    "        position = torch.arange(0, sequence_length, dtype=torch.float, device=tensor.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.model_dim, 2).float() * -(math.log(10000.0) / self.model_dim)).to(tensor.device)\n",
    "        position_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        position_encoding = position_encoding.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        return tensor + position_encoding\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, model_dim, head_count, ff_dim, dropout_rate):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = AttentionHead(model_dim, head_count)\n",
    "        self.feed_forward = FeedForwardNetwork(model_dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, tensor, mask):\n",
    "        attention_output = self.attention(tensor, tensor, tensor, mask)\n",
    "        tensor = self.norm1(tensor + self.dropout(attention_output))\n",
    "        ff_output = self.feed_forward(tensor)\n",
    "        tensor = self.norm2(tensor + self.dropout(ff_output))\n",
    "        return tensor\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, model_dim, head_count, ff_dim, dropout_rate):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attention = AttentionHead(model_dim, head_count)\n",
    "        self.cross_attention = AttentionHead(model_dim, head_count)\n",
    "        self.feed_forward = FeedForwardNetwork(model_dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.norm3 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, tensor, encoder_output, source_mask, target_mask):\n",
    "        self_attention_output = self.self_attention(tensor, tensor, tensor, target_mask)\n",
    "        tensor = self.norm1(tensor + self.dropout(self_attention_output))\n",
    "        cross_attention_output = self.cross_attention(tensor, encoder_output, encoder_output, source_mask)\n",
    "        tensor = self.norm2(tensor + self.dropout(cross_attention_output))\n",
    "        ff_output = self.feed_forward(tensor)\n",
    "        tensor = self.norm3(tensor + self.dropout(ff_output))\n",
    "        return tensor\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, model_dim, head_count, layer_count, ff_dim, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.source_embedding = nn.Embedding(source_vocab_size, model_dim)\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, model_dim)\n",
    "        self.position_encoder = PositionalEncoder(model_dim)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(model_dim, head_count, ff_dim, dropout_rate) for _ in range(layer_count)])\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(model_dim, head_count, ff_dim, dropout_rate) for _ in range(layer_count)])\n",
    "\n",
    "        self.final_linear = nn.Linear(model_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def create_mask(self, source, target):\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(3)\n",
    "        sequence_length = target.size(1)\n",
    "\n",
    "        # Ensure future_mask is created on the same device as target\n",
    "        future_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length, device=target.device), diagonal=1)).bool()\n",
    "\n",
    "        target_mask = target_mask & future_mask\n",
    "        return source_mask, target_mask\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source_mask, target_mask = self.create_mask(source, target)\n",
    "        source_embedded = self.dropout(self.position_encoder(self.source_embedding(source)))\n",
    "        target_embedded = self.dropout(self.position_encoder(self.target_embedding(target)))\n",
    "\n",
    "        encoder_output = source_embedded\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            encoder_output = encoder_block(encoder_output, source_mask)\n",
    "\n",
    "        decoder_output = target_embedded\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            decoder_output = decoder_block(decoder_output, encoder_output, source_mask, target_mask)\n",
    "\n",
    "        final_output = self.final_linear(decoder_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ddcf49",
   "metadata": {
    "id": "04ddcf49"
   },
   "source": [
    "## Breakdown of the Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56180380",
   "metadata": {
    "id": "56180380"
   },
   "source": [
    "- `Embeddings:` Converts input tokens into fixed-size vectors.\n",
    "\n",
    "- `Positional Encoding:` Adds information about the position of tokens in the sequence.\n",
    "\n",
    "- `Encoder Blocks:` Each block applies self-attention to the source sequence and passes the result through a feed-forward network. Normalization and dropout are included for stability and regularization.\n",
    "\n",
    "- `Decoder Blocks:` Similar to encoder blocks but with an additional cross-attention layer that focuses on the encoder's output. Includes self-attention, cross-attention, a feed-forward network, normalization, and dropout.\n",
    "\n",
    "- `Masks:` Source masks ignore padding, and target masks ensure predictions are based only on previously seen tokens.\n",
    "\n",
    "- `Final Linear Layer:` Transforms decoder output to the target vocabulary size for probability predictions.\n",
    "\n",
    "- `Output:` The model outputs a sequence of probability distributions over the target vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd81c09",
   "metadata": {
    "id": "4cd81c09"
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902b7828",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "902b7828",
    "outputId": "05a33471-efca-4412-be3c-394335fc6663"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\"\"\"Step 1: Load the Data from the text files.\"\"\"\n",
    "def load_data(en_file, de_file):\n",
    "    with open(en_file, 'r', encoding='utf-8') as f:\n",
    "        english_sentences = f.readlines()\n",
    "    with open(de_file, 'r', encoding='utf-8') as f:\n",
    "        german_sentences = f.readlines()\n",
    "    return english_sentences, german_sentences\n",
    "\n",
    "english_sentences, german_sentences = load_data(en_file, de_file)\n",
    "\n",
    "\"\"\"Step 2: Tokenization Tokenize the sentences into words..\"\"\"\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer):\n",
    "    for sentence in data_iter:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "def build_vocab(sentences, tokenizer):\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(sentences, tokenizer), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "    vocab.set_default_index(vocab['<unk>'])  # Set '<unk>' as the default index for unknown tokens\n",
    "    return vocab\n",
    "\n",
    "en_vocab = build_vocab(english_sentences, en_tokenizer)\n",
    "de_vocab = build_vocab(german_sentences, de_tokenizer)\n",
    "\n",
    "src_vocab_size = len(en_vocab)\n",
    "tgt_vocab_size = len(de_vocab)\n",
    "# print(src_vocab_size,tgt_vocab_size)\n",
    "\n",
    "\"\"\"Step 3: Convert the tokenized sentences to integer sequences.\"\"\"\n",
    "def tokenize_and_convert_to_ints(sentences, tokenizer, vocab):\n",
    "    token_ids = []\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        token_ids.append([vocab['<bos>']] + [vocab[token] for token in tokens] + [vocab['<eos>']])\n",
    "    return token_ids\n",
    "\n",
    "en_token_ids = tokenize_and_convert_to_ints(english_sentences, en_tokenizer, en_vocab)\n",
    "de_token_ids = tokenize_and_convert_to_ints(german_sentences, de_tokenizer, de_vocab)\n",
    "\n",
    "\"\"\"Step 4: Padding Sequences\"\"\"\n",
    "\n",
    "def pad_sequences(token_ids, pad_index):\n",
    "    return pad_sequence([torch.tensor(s) for s in token_ids], padding_value=pad_index, batch_first=True)\n",
    "\n",
    "en_padded = pad_sequences(en_token_ids, en_vocab['<pad>'])\n",
    "de_padded = pad_sequences(de_token_ids, de_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76312927",
   "metadata": {
    "id": "76312927"
   },
   "source": [
    "# Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e28aeca6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "e28aeca6",
    "outputId": "53c988bd-6c5f-4fb8-e445-70a8bcedfdbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (source_embedding): Embedding(18681, 512)\n",
       "  (target_embedding): Embedding(35731, 512)\n",
       "  (position_encoder): PositionalEncoder()\n",
       "  (encoder_blocks): ModuleList(\n",
       "    (0-1): 2 x EncoderBlock(\n",
       "      (attention): AttentionHead(\n",
       "        (query_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardNetwork(\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0-1): 2 x DecoderBlock(\n",
       "      (self_attention): AttentionHead(\n",
       "        (query_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attention): AttentionHead(\n",
       "        (query_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_weight): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardNetwork(\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_linear): Linear(in_features=512, out_features=35731, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "BATCH_SIZE = 32\n",
    "validation_split_percentage = 0.2\n",
    "d_model = 512\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "d_ff = 1024\n",
    "max_seq_length = 66\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0001\n",
    "\n",
    "total_size = len(en_padded)\n",
    "val_size = int(total_size * validation_split_percentage)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "# Create the combined dataset\n",
    "full_dataset = TensorDataset(en_padded, de_padded)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for both datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Instantiate the transformer model\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, dropout)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Move the model to device\n",
    "transformer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f8045",
   "metadata": {
    "id": "230f8045"
   },
   "source": [
    "## Description of Hyperparamters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fdb415",
   "metadata": {
    "id": "56fdb415"
   },
   "source": [
    "- `num_epochs`: 100  \n",
    "  The number of training cycles through the entire dataset.\n",
    "\n",
    "- `BATCH_SIZE`: 32  \n",
    "  The number of samples that will be propagated through the network in one forward/backward pass.\n",
    "\n",
    "- `validation_split_percentage`: 0.2  \n",
    "  The fraction of the dataset to be used as validation data.\n",
    "\n",
    "- `d_model`: 512  \n",
    "  The number of expected features in the encoder/decoder inputs (dimensionality of the embeddings).\n",
    "\n",
    "- `num_heads`: 4  \n",
    "  The number of heads in the multi-head attention models.\n",
    "\n",
    "- `num_layers`: 2  \n",
    "  The number of sub-encoder/decoder layers in the transformer.\n",
    "\n",
    "- `d_ff`: 1024  \n",
    "  The dimension of the feed-forward network model.\n",
    "\n",
    "- `max_seq_length`: 66  \n",
    "  The maximum length of the input sequences.\n",
    "\n",
    "- `dropout`: 0.1  \n",
    "  The dropout value is a regularization parameter.\n",
    "\n",
    "- `learning_rate`: 0.0001  \n",
    "  The step size at each iteration while moving toward a minimum of the loss function.\n",
    "\n",
    "- `optimizer`: Adam  \n",
    "  An optimization algorithm that can handle sparse gradients on noisy problems. It uses the square of gradients to scale the learning rate and takes advantage of momentum by using the moving average of the gradient instead of gradient itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f370c3",
   "metadata": {
    "id": "87f370c3"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0fd8da",
   "metadata": {
    "id": "bf0fd8da"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Initialize scheduler and early stopping parameters\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1, verbose=True)\n",
    "early_stopping_patience = 10\n",
    "early_stopping_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    transformer.train()\n",
    "    total_loss = 0\n",
    "    for source, target in train_dataloader:\n",
    "        source, target = source.to(device), target.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = transformer(source, target[:, :-1])\n",
    "\n",
    "        # Compute loss; we need to reshape the output and target\n",
    "        loss = criterion(output.reshape(-1, output.shape[-1]), target[:, 1:].reshape(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(transformer.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    training_losses.append(average_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {average_loss:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    transformer.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for source, target in validation_dataloader:\n",
    "            source, target = source.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = transformer(source, target[:, :-1])\n",
    "\n",
    "            # Compute loss\n",
    "            val_loss = criterion(output.reshape(-1, output.shape[-1]), target[:, 1:].reshape(-1))\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    average_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    validation_losses.append(average_val_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {average_val_loss:.4f}')\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(average_val_loss)\n",
    "\n",
    "    # Early stopping and checkpointing for the best model\n",
    "    if average_val_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {average_val_loss:.4f}. Saving checkpoint...')\n",
    "        best_val_loss = average_val_loss\n",
    "        best_model_state = transformer.state_dict()\n",
    "\n",
    "        # Checkpoint for resuming training later\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': best_model_state,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'early_stopping_counter': early_stopping_counter,\n",
    "            'training_losses': training_losses,\n",
    "            'validation_losses': validation_losses\n",
    "        }\n",
    "\n",
    "        # Use os.path.join to create the full path for the checkpoint\n",
    "        best_checkpoint_path = os.path.join(model_save_dir, 'transformer_best.pth')\n",
    "\n",
    "        # Save the checkpoint\n",
    "        torch.save(checkpoint, best_checkpoint_path)\n",
    "\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model state before testing or deployment\n",
    "best_model_path = os.path.join(model_save_dir, 'transformer_best.pth')\n",
    "checkpoint = torch.load(best_model_path)\n",
    "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438a04c",
   "metadata": {
    "id": "3438a04c"
   },
   "source": [
    "## Description of Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b07be7e",
   "metadata": {
    "id": "1b07be7e"
   },
   "source": [
    "The training was performed on the Europarl Parallel corpus, but only with 20,000 lines of text due to memory limitations and constant crashes on Google Colab's environment.\n",
    "1. **Setup**:\n",
    "   - A learning rate scheduler (`ReduceLROnPlateau`) is initialized to reduce the learning rate when the validation loss stops improving, helping in convergence.\n",
    "   - Early stopping parameters are set to prevent overfitting if the validation loss does not improve for a certain number of epochs (`early_stopping_patience`).\n",
    "\n",
    "2. **Training Loop**:\n",
    "   The model undergoes training for the predefined number of epochs, with each epoch consisting of both training and validation phases. (epochs were set to 100 due to reliance on Early Stopping)\n",
    "\n",
    "   - **Training Phase**:\n",
    "     - The model is set to training mode (`transformer.train()`).\n",
    "     - For each batch, the model performs a forward pass and computes the loss.\n",
    "     - Backpropagation is performed (`loss.backward()`), and gradients are clipped to prevent exploding gradients (`clip_grad_norm_`).\n",
    "     - The optimizer updates the model's weights (`optimizer.step()`).\n",
    "     - The total training loss is accumulated and logged.\n",
    "\n",
    "   - **Validation Phase**:\n",
    "     - The model is set to evaluation mode (`transformer.eval()`).\n",
    "     - Validation loss is calculated over the entire validation dataset without gradient updates.\n",
    "     - The scheduler checks the validation loss to adjust the learning rate if necessary.\n",
    "     - Validation loss is recorded and compared to the best loss seen so far.\n",
    "\n",
    "3. **Checkpointing**:\n",
    "   - If the validation loss improves, the model's state is saved (`torch.save(checkpoint, best_checkpoint_path)`).\n",
    "   - This checkpoint includes the model state, optimizer state, scheduler state, and the current best validation loss, so training can be started again from the last checkpoint.\n",
    "   - Early stopping counter is reset if there's an improvement.\n",
    "\n",
    "4. **Early Stopping Check**:\n",
    "   - If there's no improvement in validation loss for a number of epochs specified by `early_stopping_patience`, early stopping is triggered, and training is stopped.\n",
    "\n",
    "5. **Final Model Loading**:\n",
    "   - After training, the best model state is loaded from the checkpoint for testing  (`transformer.load_state_dict(checkpoint['model_state_dict'])`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe5470f",
   "metadata": {
    "id": "ebe5470f"
   },
   "source": [
    "# Plotting the Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abffccc",
   "metadata": {
    "id": "0abffccc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Assuming that training_losses and validation_losses are populated with the respective loss values\n",
    "epochs = np.arange(1, len(training_losses) + 1)\n",
    "\n",
    "# Plotting the training and validation loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(epochs, training_losses, label='Training Loss')\n",
    "\n",
    "# Plot validation loss\n",
    "plt.plot(epochs, validation_losses, label='Validation Loss', linestyle='--')\n",
    "\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Set x-ticks to start from 1\n",
    "plt.xticks(epochs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd75999",
   "metadata": {
    "id": "5dd75999"
   },
   "source": [
    "![title](https://i.imgur.com/kKd4SkV.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10554a",
   "metadata": {
    "id": "7a10554a"
   },
   "source": [
    "# Calculating BLEU score on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0641d87b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0641d87b",
    "outputId": "31ef19a3-be0d-478f-c828-8653d6fed0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on validation data: 0.0149\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "test_en_sentences, test_de_sentences = load_data(val_en_file, val_de_file)\n",
    "\n",
    "# Preprocess the test data\n",
    "test_en_token_ids = tokenize_and_convert_to_ints(test_en_sentences, en_tokenizer, en_vocab)\n",
    "test_de_token_ids = tokenize_and_convert_to_ints(test_de_sentences, de_tokenizer, de_vocab)\n",
    "\n",
    "test_en_padded = pad_sequences(test_en_token_ids, en_vocab['<pad>'])\n",
    "test_de_padded = pad_sequences(test_de_token_ids, de_vocab['<pad>'])\n",
    "\n",
    "# Create the test dataset and DataLoader\n",
    "test_dataset = TensorDataset(test_en_padded, test_de_padded)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Function to decode model output into sentences\n",
    "def decode_output(output, vocab):\n",
    "    itos = vocab.get_itos()  # Get the list of tokens (index to string)\n",
    "    sentence = [itos[token] for token in output if token not in (vocab['<bos>'], vocab['<eos>'], vocab['<pad>'])]\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Evaluate on the test set\n",
    "transformer.eval()\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "with torch.no_grad():\n",
    "    for source, target in test_dataloader:\n",
    "        source, target = source.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = transformer(source, target[:, :-1])\n",
    "\n",
    "        # Convert model output to tokens (may need to implement beam search or greedy decoding)\n",
    "        predicted_tokens = output.argmax(2)\n",
    "\n",
    "        # Decode tokens into sentences\n",
    "        predictions = [decode_output(t, de_vocab) for t in predicted_tokens]\n",
    "        references = [decode_output(t, de_vocab) for t in target]\n",
    "\n",
    "        all_predictions.extend(predictions)\n",
    "        all_references.extend(references)\n",
    "\n",
    "# Calculate BLEU score with smoothing to avoid zero scores for higher-order n-grams when there are no matches\n",
    "chencherry = SmoothingFunction()\n",
    "bleu_score = corpus_bleu(\n",
    "    [[ref.split()] for ref in all_references],  # Reference sentences should be tokenized into lists of words\n",
    "    [pred.split() for pred in all_predictions],  # Hypothesis sentences should be tokenized into lists of words\n",
    "    smoothing_function=chencherry.method1  # Use smoothing method\n",
    ")\n",
    "print(f'BLEU score on validation data: {bleu_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42429e1",
   "metadata": {
    "id": "c42429e1"
   },
   "source": [
    "# Output Translation from Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e387640",
   "metadata": {
    "id": "3e387640"
   },
   "outputs": [],
   "source": [
    "# Translate the test set and print the translations\n",
    "def translate_and_print_test_set(test_dataloader, model, de_vocab, device):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for source, _ in test_dataloader:  # No need for German sentences here\n",
    "            source = source.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(source, torch.zeros((source.size(0), 1), dtype=torch.long).to(device))\n",
    "            predicted_tokens = output.argmax(2)\n",
    "\n",
    "            # Decode tokens into sentences and print them\n",
    "            for t in predicted_tokens:\n",
    "                translation = decode_output(t[1:], de_vocab)  # Skip the <bos> token\n",
    "                print(translation)\n",
    "\n",
    "translate_and_print_test_set(test_dataloader, transformer, de_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173715d",
   "metadata": {},
   "source": [
    "# Final Comments on Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a9df8",
   "metadata": {},
   "source": [
    "Given the constraints of memory and computational resources on Google Colab's environment, the model's performance is limited by the inability to scale up the hyperparameters. With only **2 layers** and **4 attention heads**, may be too simplistic to effectively capture the complex patterns in the language translation task, but adjustments are not feasible within the current resource limitations.\n",
    "\n",
    "The dataset size of **20,000 lines**, did not provide the diversity and volume of examples needed for the model to generalize well. Language translation is a highly complex task that benefits from large datasets to cover the complexities and variances in language use. However, using a larger dataset led to further memory and computational complications.\n",
    "\n",
    "Hyperparameter optimization is a crucial step in improving model performance, specifically the batch size might be too small to provide stable gradient estimates, while typical applications use a batch size of **64**, our model was only trained on **32** to avoid memory crashes.\n",
    "\n",
    "In summary, the model's suboptimal performance can be attributed to a combination of its relatively simple architecture, the limited size of the training dataset, and the conservative hyperparameter settings, all of which are bounded by the current memory and computational bottlenecks. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
